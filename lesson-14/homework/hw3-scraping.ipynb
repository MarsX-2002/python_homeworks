{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "\n",
    "You are tasked with scraping laptop data from the \"Laptops\" section of the [Demoblaze website](https://www.demoblaze.com/) and storing the extracted information in JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opened the Demoblaze homepage...\n",
      "Clicked on the 'Laptops' category...\n",
      "Scraping current page...\n",
      "No more pages to scrape. Ending...\n",
      "Scraped 7 laptops and saved to 'laptops.json'.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# 1. Setup WebDriver and open the page\n",
    "def init_driver():\n",
    "    # Set up Selenium WebDriver for Chrome\n",
    "    options = webdriver.ChromeOptions()\n",
    "    # options.add_argument('--headless')  # Uncomment this line if you want to run the browser in headless mode\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    return driver\n",
    "\n",
    "# 2. Function to scrape data from a single page\n",
    "def scrape_laptops_page(driver):\n",
    "    time.sleep(3)  # Wait for the page to load\n",
    "\n",
    "    laptops = []\n",
    "    \n",
    "    # Find all the laptop elements on the page using the container div with the correct XPath\n",
    "    laptop_elements = driver.find_elements(By.XPATH, '//*[@id=\"tbodyid\"]/div')\n",
    "    \n",
    "    for laptop in laptop_elements:\n",
    "        name = laptop.find_element(By.CSS_SELECTOR, 'h4.card-title').text.strip()\n",
    "        price = laptop.find_element(By.CSS_SELECTOR, 'h5').text.strip()\n",
    "        description = laptop.find_element(By.CSS_SELECTOR, 'p.card-text').text.strip()\n",
    "        \n",
    "        laptops.append({\n",
    "            \"name\": name,\n",
    "            \"price\": price,\n",
    "            \"description\": description\n",
    "        })\n",
    "    \n",
    "    return laptops\n",
    "\n",
    "# 3. Function to handle pagination and scrape all laptops\n",
    "def scrape_all_laptops():\n",
    "    driver = init_driver()\n",
    "    \n",
    "    # Start by navigating to the homepage\n",
    "    driver.get(\"https://www.demoblaze.com/\")\n",
    "    print(\"Opened the Demoblaze homepage...\")\n",
    "    time.sleep(5)  # Wait for the homepage to load\n",
    "    \n",
    "    # Find and click on the \"Laptops\" category using the correct XPath\n",
    "    laptops_link = driver.find_element(By.XPATH, '//*[@id=\"itemc\"]')  # Laptops link\n",
    "    laptops_link.click()\n",
    "    print(\"Clicked on the 'Laptops' category...\")\n",
    "    time.sleep(5)  # Wait for the Laptops page to load\n",
    "    \n",
    "    all_laptops = []\n",
    "    \n",
    "    while True:\n",
    "        print(f\"Scraping current page...\")\n",
    "        \n",
    "        # Scrape laptops from the current page\n",
    "        laptops_on_page = scrape_laptops_page(driver)\n",
    "        all_laptops.extend(laptops_on_page)\n",
    "        \n",
    "        # Try to find the \"Next\" button and click it for pagination\n",
    "        try:\n",
    "            next_button = driver.find_element(By.LINK_TEXT, 'Next')\n",
    "            next_button.click()\n",
    "            print(\"Clicked on 'Next' to go to the next page...\")\n",
    "            time.sleep(5)  # Wait for the next page to load\n",
    "        except Exception as e:\n",
    "            print(\"No more pages to scrape. Ending...\")\n",
    "            break\n",
    "    \n",
    "    driver.quit()\n",
    "    return all_laptops\n",
    "\n",
    "# 4. Save the scraped data in JSON format\n",
    "def save_to_json(data, filename='laptops.json'):\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "# 5. Main function to scrape and save the data\n",
    "def main():\n",
    "    # Scrape all laptops from the site\n",
    "    all_laptops = scrape_all_laptops()\n",
    "    \n",
    "    # Save the scraped data to a JSON file\n",
    "    save_to_json(all_laptops)\n",
    "    \n",
    "    print(f\"Scraped {len(all_laptops)} laptops and saved to 'laptops.json'.\")\n",
    "\n",
    "# Run the script\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
